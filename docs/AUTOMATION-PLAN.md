# ğŸ¤– Automation & Data Crawling Plan

## ğŸ“‹ Tá»•ng Quan

Website sáº½ tá»± Ä‘á»™ng cáº­p nháº­t ná»™i dung háº±ng ngÃ y tá»« nhiá»u nguá»“n khÃ¡c nhau, sá»­ dá»¥ng Supabase lÃ m database chÃ­nh vÃ  AI Ä‘á»ƒ phÃ¢n loáº¡i/tá»‘i Æ°u dá»¯ liá»‡u.

---

## ğŸ—„ï¸ Supabase Database Schema

### Table 1: `resources`
```sql
CREATE TABLE resources (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  title TEXT NOT NULL,
  description TEXT,
  url TEXT NOT NULL UNIQUE,
  category TEXT NOT NULL, -- 'ui-kits', 'icons', 'illustrations', 'photos', etc.
  tags TEXT[], -- Array of tags
  pricing TEXT, -- 'Free', 'Freemium', 'Premium', 'Free Trial'
  emoji TEXT,
  gradient TEXT,
  source TEXT, -- 'producthunt', 'undesign', 'manual'
  featured BOOLEAN DEFAULT false,
  created_at TIMESTAMPTZ DEFAULT now(),
  updated_at TIMESTAMPTZ DEFAULT now(),
  last_crawled TIMESTAMPTZ
);

-- Indexes for performance
CREATE INDEX idx_resources_category ON resources(category);
CREATE INDEX idx_resources_featured ON resources(featured);
CREATE INDEX idx_resources_created_at ON resources(created_at DESC);
```

### Table 2: `inspirations`
```sql
CREATE TABLE inspirations (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  title TEXT NOT NULL,
  description TEXT,
  image_url TEXT,
  source_url TEXT NOT NULL UNIQUE,
  category TEXT NOT NULL, -- 'web', 'mobile', 'dashboard', 'branding', 'illustration'
  tags TEXT[],
  emoji TEXT,
  gradient TEXT,
  source TEXT, -- 'bookmarks.design', 'dribbble', 'behance', 'awwwards'
  featured BOOLEAN DEFAULT false,
  created_at TIMESTAMPTZ DEFAULT now(),
  updated_at TIMESTAMPTZ DEFAULT now(),
  last_crawled TIMESTAMPTZ
);

CREATE INDEX idx_inspirations_category ON inspirations(category);
CREATE INDEX idx_inspirations_created_at ON inspirations(created_at DESC);
```

### Table 3: `videos`
```sql
CREATE TABLE videos (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  title TEXT NOT NULL,
  description TEXT,
  youtube_id TEXT NOT NULL UNIQUE,
  url TEXT NOT NULL,
  channel_name TEXT,
  channel_id TEXT,
  category TEXT NOT NULL, -- 'fundamentals', 'tools', 'ui-ux', 'web', 'advanced'
  duration TEXT, -- '12:34' format
  thumbnail_url TEXT,
  emoji TEXT,
  gradient TEXT,
  view_count INTEGER,
  published_at TIMESTAMPTZ,
  created_at TIMESTAMPTZ DEFAULT now(),
  updated_at TIMESTAMPTZ DEFAULT now(),
  last_crawled TIMESTAMPTZ
);

CREATE INDEX idx_videos_category ON videos(category);
CREATE INDEX idx_videos_published_at ON videos(published_at DESC);
```

### Table 4: `articles`
```sql
CREATE TABLE articles (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  title TEXT NOT NULL,
  description TEXT,
  url TEXT NOT NULL UNIQUE,
  author TEXT,
  source TEXT, -- 'Medium', 'Smashing Magazine', etc.
  category TEXT,
  tags TEXT[],
  emoji TEXT,
  gradient TEXT,
  published_at TIMESTAMPTZ,
  created_at TIMESTAMPTZ DEFAULT now(),
  updated_at TIMESTAMPTZ DEFAULT now(),
  last_crawled TIMESTAMPTZ
);

CREATE INDEX idx_articles_created_at ON articles(created_at DESC);
```

### Table 5: `crawl_logs`
```sql
CREATE TABLE crawl_logs (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  source TEXT NOT NULL, -- 'bookmarks.design', 'youtube:figma', etc.
  table_name TEXT NOT NULL, -- 'resources', 'inspirations', 'videos', 'articles'
  status TEXT NOT NULL, -- 'success', 'failed', 'partial'
  items_added INTEGER DEFAULT 0,
  items_updated INTEGER DEFAULT 0,
  error_message TEXT,
  started_at TIMESTAMPTZ DEFAULT now(),
  completed_at TIMESTAMPTZ
);

CREATE INDEX idx_crawl_logs_source ON crawl_logs(source);
CREATE INDEX idx_crawl_logs_started_at ON crawl_logs(started_at DESC);
```

---

## ğŸ•·ï¸ Crawling Strategy

### 1. Design Inspiration (bookmarks.design, Dribbble, Behance)

**Táº§n suáº¥t:** 3 láº§n/ngÃ y (8:00, 14:00, 20:00)

**Dá»¯ liá»‡u cáº§n crawl:**
- Title
- Image URL
- Source URL
- Category (tá»± Ä‘á»™ng phÃ¢n loáº¡i báº±ng AI)
- Tags

**CÃ´ng cá»¥:**
- Puppeteer/Playwright cho dynamic content
- Cheerio cho static HTML
- OpenAI API Ä‘á»ƒ phÃ¢n loáº¡i category & generate description

**Flow:**
```
1. Crawl website â†’ Extract raw data
2. Check duplicate (by URL) trong Supabase
3. Náº¿u má»›i â†’ AI phÃ¢n loáº¡i category + generate description
4. AI chá»n emoji + gradient phÃ¹ há»£p
5. Insert vÃ o Supabase
6. Log vÃ o crawl_logs
```

### 2. YouTube Videos

**Táº§n suáº¥t:** 2 láº§n/ngÃ y (10:00, 18:00)

**KÃªnh Æ°u tiÃªn:**
- Figma, DesignCourse, Flux Academy, The Futur, Jesse Showalter

**API:** YouTube Data API v3

**Dá»¯ liá»‡u cáº§n crawl:**
- Video title, description
- Video ID, URL
- Channel name, ID
- Duration, thumbnail
- View count, publish date

**Flow:**
```
1. Gá»i YouTube API vá»›i channel IDs
2. Láº¥y videos má»›i nháº¥t (published trong 7 ngÃ y qua)
3. Check duplicate by youtube_id
4. AI phÃ¢n loáº¡i category (fundamentals, tools, ui-ux, etc.)
5. AI chá»n emoji + gradient
6. Insert vÃ o Supabase
```

### 3. Design Resources (Product Hunt, Undesign)

**Táº§n suáº¥t:** 1 láº§n/ngÃ y (9:00)

**Dá»¯ liá»‡u cáº§n crawl:**
- Tool/resource name
- Description
- URL
- Pricing (Free/Premium/Freemium)
- Category

**API:**
- Product Hunt API (cÃ³ official API)
- Web scraping cho cÃ¡c site khÃ¡c

### 4. Articles (Medium, Smashing Magazine)

**Táº§n suáº¥t:** 1 láº§n/ngÃ y (11:00)

**Dá»¯ liá»‡u cáº§n crawl:**
- Title, description
- Author, source
- URL, publish date
- Tags

---

## ğŸ¤– AI Automation Tasks

### 1. Auto-Categorization
```typescript
// Sá»­ dá»¥ng OpenAI Ä‘á»ƒ phÃ¢n loáº¡i
const categorizeContent = async (title: string, description: string) => {
  const prompt = `
  PhÃ¢n loáº¡i ná»™i dung design nÃ y vÃ o 1 trong cÃ¡c category:
  - ui-kits, icons, illustrations, photos, typography, colors
  - design-tools, ai, accessibility, prototyping
  - patterns, courses, articles

  Title: ${title}
  Description: ${description}

  Tráº£ vá» JSON: { "category": "...", "tags": ["tag1", "tag2"] }
  `;

  const response = await openai.chat.completions.create({
    model: "gpt-4",
    messages: [{ role: "user", content: prompt }]
  });

  return JSON.parse(response.choices[0].message.content);
};
```

### 2. Auto-Generate Description
```typescript
// Tá»‘i Æ°u description cho SEO
const generateDescription = async (title: string, rawContent: string) => {
  const prompt = `
  Viáº¿t mÃ´ táº£ ngáº¯n gá»n (1-2 cÃ¢u, max 150 chars) cho design resource nÃ y:
  Title: ${title}
  Content: ${rawContent}

  MÃ´ táº£ pháº£i háº¥p dáº«n, rÃµ rÃ ng, vÃ  SEO-friendly.
  `;

  const response = await openai.chat.completions.create({
    model: "gpt-4",
    messages: [{ role: "user", content: prompt }]
  });

  return response.choices[0].message.content;
};
```

### 3. Auto-Select Emoji & Gradient
```typescript
const selectEmojiAndGradient = async (category: string, title: string) => {
  const gradients = {
    'ui-kits': 'linear-gradient(135deg, #ffecd2 0%, #fcb69f 100%)',
    'icons': 'linear-gradient(135deg, #d299c2 0%, #fef9d7 100%)',
    'illustrations': 'linear-gradient(135deg, #fbc2eb 0%, #a6c1ee 100%)',
    // ... more gradients
  };

  const emojis = {
    'ui-kits': 'ğŸ¨',
    'icons': 'â­',
    'illustrations': 'ğŸ­',
    // ... more emojis
  };

  return {
    emoji: emojis[category] || 'âœ¨',
    gradient: gradients[category] || 'linear-gradient(135deg, #a8edea 0%, #fed6e3 100%)'
  };
};
```

---

## â° Cron Jobs Schedule (Using Vercel Cron hoáº·c Supabase Edge Functions)

```javascript
// vercel.json
{
  "crons": [
    {
      "path": "/api/cron/crawl-inspirations",
      "schedule": "0 8,14,20 * * *" // 3 láº§n/ngÃ y: 8am, 2pm, 8pm
    },
    {
      "path": "/api/cron/crawl-youtube",
      "schedule": "0 10,18 * * *" // 2 láº§n/ngÃ y: 10am, 6pm
    },
    {
      "path": "/api/cron/crawl-resources",
      "schedule": "0 9 * * *" // 1 láº§n/ngÃ y: 9am
    },
    {
      "path": "/api/cron/crawl-articles",
      "schedule": "0 11 * * *" // 1 láº§n/ngÃ y: 11am
    },
    {
      "path": "/api/cron/cleanup-old-data",
      "schedule": "0 2 * * 0" // 1 láº§n/tuáº§n: Chá»§ nháº­t 2am - xÃ³a data cÅ© >6 thÃ¡ng
    }
  ]
}
```

---

## ğŸ› ï¸ Tech Stack cho Automation

### Backend (API Routes in Next.js)
```
/api/cron/
  â”œâ”€â”€ crawl-inspirations.ts    // Crawl bookmarks.design, Dribbble, etc.
  â”œâ”€â”€ crawl-youtube.ts          // Crawl YouTube videos
  â”œâ”€â”€ crawl-resources.ts        // Crawl Product Hunt, Undesign
  â”œâ”€â”€ crawl-articles.ts         // Crawl Medium, Smashing Magazine
  â””â”€â”€ cleanup-old-data.ts       // Dá»n dáº¹p data cÅ©
```

### Dependencies cáº§n thÃªm
```json
{
  "dependencies": {
    "cheerio": "^1.0.0",           // HTML parsing
    "puppeteer": "^21.0.0",        // Dynamic content crawling
    "axios": "^1.6.0",             // HTTP requests
    "openai": "^4.20.0",           // AI categorization
    "youtube-transcript": "^1.0.6" // YouTube transcripts (optional)
  }
}
```

---

## ğŸ“Š Dashboard Ä‘á»ƒ Monitor Crawling

Táº¡o admin page táº¡i `/admin` Ä‘á»ƒ:
- Xem crawl logs realtime
- Trigger manual crawl
- Xem stats: sá»‘ items má»›i/ngÃ y, success rate
- Approve/reject auto-generated content

---

## ğŸš€ Implementation Phases

### Phase 1: Setup Database (Week 1)
- [ ] Táº¡o Supabase tables
- [ ] Setup Row Level Security (RLS)
- [ ] Táº¡o API helpers

### Phase 2: Crawlers (Week 2-3)
- [ ] Implement bookmarks.design crawler
- [ ] Implement YouTube crawler vá»›i API
- [ ] Implement Product Hunt crawler
- [ ] Test & refine

### Phase 3: AI Integration (Week 4)
- [ ] Setup OpenAI API
- [ ] Auto-categorization
- [ ] Auto-description generation
- [ ] Auto emoji/gradient selection

### Phase 4: Automation (Week 5)
- [ ] Setup Vercel Cron Jobs
- [ ] Implement error handling & retry logic
- [ ] Setup email alerts cho failed crawls
- [ ] Create admin dashboard

### Phase 5: Monitoring & Optimization (Week 6)
- [ ] Monitor performance
- [ ] Optimize crawl speed
- [ ] Add rate limiting
- [ ] A/B test AI prompts

---

## ğŸ’° Cost Estimation (Monthly)

- **Supabase**: Free tier (up to 500MB database)
- **OpenAI API**: ~$20-50/month (phá»¥ thuá»™c sá»‘ lÆ°á»£ng items)
- **YouTube API**: Free (10,000 quota/day)
- **Vercel Cron**: Free (included in Hobby plan)

**Total: ~$20-50/month**

---

## âš ï¸ Considerations

1. **Rate Limiting**: TrÃ¡nh bá»‹ block bá»Ÿi websites
   - ThÃªm delays giá»¯a requests
   - Rotate user agents
   - Respect robots.txt

2. **Duplicate Detection**: Check URL uniqueness trÆ°á»›c khi insert

3. **Data Quality**:
   - AI cÃ³ thá»ƒ sai category â†’ cáº§n manual review
   - Táº¡o admin interface Ä‘á»ƒ approve/reject

4. **Legal**:
   - Chá»‰ crawl public data
   - Respect copyright
   - Link back to original sources

---

## ğŸ“ Next Steps

Cáº­u muá»‘n tá»› báº¯t Ä‘áº§u implement pháº§n nÃ o trÆ°á»›c?
1. Setup Supabase database schema?
2. Táº¡o crawler Ä‘áº§u tiÃªn (bookmarks.design)?
3. Setup YouTube API integration?
4. Setup OpenAI auto-categorization?
